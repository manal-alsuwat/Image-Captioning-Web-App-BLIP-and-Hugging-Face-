# üñºÔ∏è Image Captioning Web App (BLIP + Hugging Face)

This project is a simple web application that generates captions for uploaded images using the **BLIP** (Bootstrapping Language-Image Pretraining) model from **Hugging Face**.  
It demonstrates how computer vision and natural language processing can work together to describe images in human-like language.

---

## üöÄ Features
- Upload any image and get an AI-generated caption.  
- Powered by the **BLIP** model for image-to-text understanding.  
- User-friendly web interface built with **Gradio**.  

---

## üß† Technologies & Libraries Used
- **Python 3.x**  
- **Transformers** (for loading BLIP model and processor)  
- **Torch / Torchvision** (for tensor processing)  
- **Pillow (PIL)** (for image handling)  
- **Gradio** (for creating the web interface)  

---

## üß© Model Source
- [Salesforce/blip-image-captioning-base](https://huggingface.co/Salesforce/blip-image-captioning-base)

<img width="1186" height="477" alt="image" src="https://github.com/user-attachments/assets/ebf5b24e-aac0-414b-a271-56d573ce07fa" />


